streamlit>=1.36.0
flask>=3.0.3
fastapi>=0.111.1
uvicorn>=0.30.1
py-cpuinfo>=9.0.0
line_profiler>=4.1.3
locust>=2.29.1
requests>=2.32.3
langchain>=0.2.5
wandb>=0.17.3
tiktoken>=0.6.0
blobfile>=2.1.1
tokenizers>=0.15.2
trl>=0.9.6
fairscale>=0.4.13
fire>=0.6.0
diffusers==0.16.1
transformers>=4.36.2
accelerate>=0.25.0
peft>=0.11.1
ray[serve]>=2.32.0
onnx>=1.16.1
onnxruntime>=1.18.1
openvino>=2024.2.0
optimum==0.1.0
optimum[onnxruntime] # @git+https://github.com/huggingface/optimum.git
optimum[ipex] # @git+https://github.com/huggingface/optimum-intel.git
optimum[openvino] # @git+https://github.com/huggingface/optimum-intel.git
optimum[neuronx] # @git+https://github.com/huggingface/optimum-neuron.git
optimum[habana] # @git+https://github.com/huggingface/optimum-habana.git
optimum[furiosa] # @git+https://github.com/huggingface/optimum-furiosa.git
optimum[amd] # @git+https://github.com/huggingface/optimum-amd.git
optimum[neural-compressor] # @git+https://github.com/huggingface/optimum-intel.git
optimum[nncf] # @git+https://github.com/huggingface/optimum-intel.git
optimum-intel[extras] # @git+https://github.com/huggingface/optimum-intel.git
onnx2pytorch>=0.4.1
pyyaml>=6.0.1
setuptools-rust>=1.9.0
protobuf>=4.25.3
nncf>=2.11.0
vllm>=0.2.7
bitsandbytes>=0.43.1
--extra-index-url https://pypi.nvidia.com
optimum-nvidia>=0.1.0b7
--extra-index-url https://download.pytorch.org/whl/cu121
torch==2.2.2+cu121
torchvision==0.17.2+cu121
torchaudio==2.2.2+cu121
--extra-index-url https://pytorch-extension.intel.com/release-whl/stable/xpu/us/
ipex-llm==2.1.0b20240718
--extra-index-url https://developer.intel.com/ipex-whl-stable-xpu/
intel-extension-for-pytorch==2.1.30+xpu
oneccl_bind_pt==2.1.300+xpu
# --extra-index-url https://pip.repos.neuron.amazonaws.com
# neuronx-cc>=2.13.66
# torch-neuronx>=2.1.2.2.1.0
# libneuronxla>=2.0.2335
# transformers_neuronx>=0.9.474
# aws-neuronx-runtime-discovery>=1.0