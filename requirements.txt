streamlit>=1.36.0
flask>=3.0.3
fastapi>=0.111.1
anthropic>=0.38.0
openai>=1.53.0
uvicorn>=0.30.1
py-cpuinfo>=9.0.0
line_profiler>=4.1.3
locust>=2.29.1
requests>=2.32.3
langchain>=0.2.5
wandb>=0.17.3
tiktoken>=0.6.0
blobfile>=2.1.1
tokenizers>=0.15.2
langid>=1.1.6
# trl>=0.9.6
fairscale>=0.4.13
fire>=0.6.0
diffusers>=0.15.0
msgpack>=1.1.0
# flash-attn @https://github.com/Dao-AILab/flash-attention/releases/download/v2.6.3/flash_attn-2.6.3+cu118torch2.1cxx11abiFALSE-cp310-cp310-linux_x86_64.whl
transformer-engine>=1.11.0
deepspeed==0.15.2
transformers[deepspeed]>=4.41.0
sentence-transformers>=3.3.1
huggingface-hub==0.26.2
# @git+https://github.com/huggingface/transformers.git
accelerate>=1.2.0
peft>=0.11.1
ray[serve]>=2.32.0
onnx>=1.17.0
onnxruntime>=1.20.0
openvino>=2024.3.0
optimum @git+https://github.com/huggingface/optimum.git
# optimum[onnxruntime] # @git+https://github.com/huggingface/optimum.git
# optimum[ipex] # @git+https://github.com/huggingface/optimum-intel.git
optimum[openvino] # @git+https://github.com/huggingface/optimum-intel.git
# optimum[neuronx] # @git+https://github.com/huggingface/optimum-neuron.git
# optimum[habana] # @git+https://github.com/huggingface/optimum-habana.git
# optimum[furiosa] # @git+https://github.com/huggingface/optimum-furiosa.git
# optimum[amd] # @git+https://github.com/huggingface/optimum-amd.git
# optimum[neural-compressor] # @git+https://github.com/huggingface/optimum-intel.git
# optimum[nncf] # @git+https://github.com/huggingface/optimum-intel.git
# optimum-intel # @git+https://github.com/huggingface/optimum-intel.git
onnx2pytorch>=0.4.1
#pyyaml>=6.0.1 (기존)
pyyaml>=5.1
setuptools-rust>=1.9.0
protobuf>=4.25.3
nncf>=2.11.0
vllm==0.6.6
bitsandbytes>=0.43.1
--extra-index-url https://pypi.nvidia.com
# optimum-nvidia>=0.1.0b7
# tensorrt_llm>=0.9.0
--extra-index-url https://download.pytorch.org/whl/cu121
torch==2.5.1+cu121
torchvision==0.20.1+cu121
torchtext==0.18.0
torchaudio==2.5.1+cu121
torch-tensorrt==2.5.0
--extra-index-url https://pytorch-extension.intel.com/release-whl/stable/xpu/us/
ipex-llm==2.2.0b20241106
kss==6.0.4
stanza==1.10.1

# --extra-index-url https://developer.intel.com/ipex-whl-stable-xpu/
# intel-extension-for-pytorch==2.1.30+xpu
# oneccl_bind_pt==2.1.300+xpu
# --extra-index-url https://pip.repos.neuron.amazonaws.com
# neuronx-cc>=2.13.66
# torch-neuronx>=2.1.2.2.1.0
# libneuronxla>=2.0.2335
# transformers_neuronx>=0.9.474
# aws-neuronx-runtime-discovery>=1.0