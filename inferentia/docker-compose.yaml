version: "3.10"

services:
  app:
    env_file:
      - .env
    build:
      context: .
      dockerfile: Dockerfile
      args:
        - HF_TOKEN=$HF_TOKEN
        - VLLM_TARGET_DEVICE=$VLLM_TARGET_DEVICE
    volumes:
      - .:/app
      # - /opt/aws/neuron/lib:/opt/aws/neuron/lib
    devices:
      - /dev/neuron0:/dev/neuron0
    ports:
      - target: 8000       ## 컨테이너 내부 포트
        published: 8000    ## 호스트OS에서 공개할 포트
        protocol: tcp      ## 포트 프로토콜
    environment:
      - FLASK_ENV=development
      - VLLM_MODEL=TinyLlama/TinyLlama-1.1B-Chat-v1.0
    command:
      # - serve run neuron_server:build_app model=TinyLlama/TinyLlama-1.1B-Chat-v1.0 device=neuron dtype=float16 gpu_memory_utilization=0.95 max_model_len=127
      # - gunicorn deploy:app --workers=1 --worker-class=uvicorn.workers.UvicornWorker --bind=0.0.0.0:8000
      - uvicorn deploy:app --host 0.0.0.0 --port 8000

      
