version: "3.10"

services:
  app:
    env_file:
      - .env
    build:
      context: .
      dockerfile: Dockerfile
      args:
        - HF_TOKEN=$HF_TOKEN
    volumes:
      - .:/app
      - /opt/aws/neuron/lib:/opt/aws/neuron/lib
    ports:
      - "8000:8000"
    environment:
      - FLASK_ENV=development
      - VLLM_MODE=TinyLlama/TinyLlama-1.1B-Chat-v1.0
    command: 
     -| 
      gunicorn deploy:app \
          --workers 1 \
          --worker-class uvicorn.workers.UvicornWorker \
          --bind 0.0.0.0:8000
      serve run main:build_app\
          model=$VLLM_MODEL\
          device="neruon"\
          dtype="bfloat16"\
          gpu_memory_utilization=0.95\
          max_model_len=127
        
